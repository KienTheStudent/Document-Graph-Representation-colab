{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a0f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = os.getenv('NEO4J_URI')\n",
    "username = 'neo4j'\n",
    "password = os.getenv('NEO4J_AUTH')\n",
    "\n",
    "from neo4j import GraphDatabase, Result\n",
    "\n",
    "driver = GraphDatabase.driver(url, auth=(username, password), keep_alive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f37d242",
   "metadata": {},
   "source": [
    "#### Export Node relationship into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15b6eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_all():\n",
    "\n",
    "    triples_query = \"\"\"\n",
    "    MATCH (h:Test_embedding)-[r]->(t:Test_embedding)\n",
    "    RETURN h.id AS head, type(r) AS rel, t.id AS tail\n",
    "    \"\"\"\n",
    "\n",
    "    with driver.session() as session:\n",
    "        result = session.run(triples_query)\n",
    "\n",
    "        with open('triple.csv', \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"head\", \"relation\", \"tail\"])\n",
    "\n",
    "            for record in result:\n",
    "                writer.writerow([\n",
    "                    record[\"head\"],\n",
    "                    record[\"rel\"],\n",
    "                    record[\"tail\"]\n",
    "                ])\n",
    "                \n",
    "export_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109b387",
   "metadata": {},
   "source": [
    "#### Train ComplEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493af5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.pipeline.api:No random seed is specified. Setting to 230020014.\n",
      "WARNING:pykeen.utils:No cuda devices were available. The model runs on CPU\n",
      "INFO:pykeen.pipeline.api:Using device: None\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "INFO:pykeen.nn.representation:Inferred unique=False for Embedding(\n",
      "  (regularizer): LpRegularizer()\n",
      ")\n",
      "d:\\miniconda3\\envs\\phobert_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Training epochs on cpu: 100%|██████████| 100/100 [00:51<00:00,  1.93epoch/s, loss=0.919, prev_loss=0.943]\n",
      "WARNING:pykeen.utils:Using automatic batch size on device.type='cpu' can cause unexplained out-of-memory crashes. Therefore, we use a conservative small batch_size=32. Performance may be improved by explicitly specifying a larger batch size.\n",
      "Evaluating on cpu:   0%|          | 0.00/3.48k [00:00<?, ?triple/s]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n",
      "Evaluating on cpu: 100%|██████████| 3.48k/3.48k [00:01<00:00, 1.92ktriple/s]\n",
      "INFO:pykeen.evaluation.evaluator:Evaluation took 1.90s seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: 2368\n",
      "ComplEx embedding dim: 100\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('triple.csv')\n",
    "\n",
    "triples = df[['head', 'relation', 'tail']].values.tolist()\n",
    "\n",
    "# Save temporary triples file in TSV format (PyKEEN requirement)\n",
    "triples_tsv = \"triples_temp.tsv\"\n",
    "df[['head', 'relation', 'tail']].to_csv(triples_tsv, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "# Create a single TriplesFactory for all splits\n",
    "tf = TriplesFactory.from_path(triples_tsv)\n",
    "\n",
    "result = pipeline(\n",
    "    model='ComplEx',\n",
    "    training=tf,\n",
    "    validation=tf,\n",
    "    testing=tf,\n",
    "    model_kwargs={'embedding_dim': 100}, \n",
    "    training_kwargs={'num_epochs': 100},\n",
    ")\n",
    "\n",
    "model = result.model\n",
    "\n",
    "# Extract entity embeddings\n",
    "entity_embeddings = model.entity_representations[0]().cpu().detach().numpy()\n",
    "entities = list(tf.entity_to_id.keys())\n",
    "\n",
    "final_embeddings = {\n",
    "    entity: entity_embeddings[idx]\n",
    "    for idx, entity in enumerate(entities)\n",
    "}\n",
    "\n",
    "print(\"Entities:\", len(final_embeddings))\n",
    "print(\"ComplEx embedding dim:\", len(next(iter(final_embeddings.values()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf1fc33",
   "metadata": {},
   "source": [
    "#### Convert Complex embedding to Real embedding by concatenating Re and Img values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e28ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embeddings = model.entity_representations[0]().cpu().detach().numpy()\n",
    "entity_ids = tf.entity_to_id \n",
    "\n",
    "def complex_to_real_vector(complex_array):\n",
    "    # complex_array is shape (dim,), dtype=complex\n",
    "    real = complex_array.real\n",
    "    imag = complex_array.imag\n",
    "    return np.concatenate([real, imag])\n",
    "\n",
    "df_complex = pd.DataFrame({\n",
    "    'node': list(entity_ids.keys()),\n",
    "    'complex_embedding': list(entity_embeddings)\n",
    "})\n",
    "\n",
    "df_complex['real_embedding'] = df_complex['complex_embedding'].apply(complex_to_real_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7efdf8",
   "metadata": {},
   "source": [
    "#### Get node information, especially original_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e00f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_query = \"\"\"\n",
    "MATCH (n:Test_embedding)\n",
    "RETURN n.id AS node,\n",
    "       n.original_embedding AS original_embedding\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    neo_data = session.run(neo_query).data()\n",
    "\n",
    "df_neo = pd.DataFrame(neo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31a245",
   "metadata": {},
   "source": [
    "#### Concat Original + Complex embedding and write back to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120f4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_neo.merge(df_complex, on='node', how='inner')\n",
    "\n",
    "df[\"combined_embedding\"] = df.apply(\n",
    "    lambda row: row[\"original_embedding\"] + row[\"real_embedding\"].tolist(),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "records = [\n",
    "    {\"node_id\": row[\"node\"], \"embedding\": row[\"combined_embedding\"]}\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "query = \"\"\"\n",
    "UNWIND $rows AS r\n",
    "MATCH (n {id: r.node_id})\n",
    "SET n.combined_embedding = r.embedding\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(query, rows=records)\n",
    "\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
