{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3836d976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\phobert_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "from rag_model.model.Final_pipeline.final_doc_processor import *\n",
    "from rag_model.model.RE.final_re import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8159226",
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert = PhoBertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bd2ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3896cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def evaluate_embedding(referenced_context: List[str], retrieved_context: List[str], threshold=0.60):\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, F1-Score, and MRR for embedding-based retrieval.\n",
    "    \"\"\"\n",
    "    referenced_set = list(set(referenced_context))\n",
    "    retrieved_set  = list(set(retrieved_context))\n",
    "\n",
    "    # --- Precision / Recall / F1 ---\n",
    "    tp = 0 \n",
    "    for ref in referenced_set:\n",
    "        max_sim = -1\n",
    "        for ret in retrieved_set:\n",
    "            ref_emb = text_embedding(ref, 1)\n",
    "            ret_emb = text_embedding(ret, 1)\n",
    "            sim_score = cosine(ref_emb, ret_emb)\n",
    "            if sim_score > max_sim:\n",
    "                max_sim = sim_score\n",
    "        if max_sim >= threshold:\n",
    "            tp += 1\n",
    "\n",
    "    precision = tp / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
    "    recall    = tp / len(referenced_set) if len(referenced_set) > 0 else 0\n",
    "    f1_score  = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # --- MRR ---\n",
    "    reciprocal_rank = 0.0\n",
    "    for rank, ret in enumerate(retrieved_set, start=1):\n",
    "        max_sim = max(\n",
    "            cosine(text_embedding(ref, 1), text_embedding(ret, 1))\n",
    "            for ref in referenced_set\n",
    "        )\n",
    "        if max_sim >= threshold:\n",
    "            reciprocal_rank = 1 / rank\n",
    "            break  # only first relevant item counts\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1_score,\n",
    "        'MRR': reciprocal_rank\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53db721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\phobert_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--sentence-transformers--distiluse-base-multilingual-cased-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 1.0, 'Recall': 0.6666666666666666, 'F1-Score': 0.8, 'MRR': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_embedding(referenced_context=['Hà Nội là thủ đô của Việt Nam', 'Hà Nội có Lăng Bác', 'TP. Hồ Chí Minh ở miền Nam'], retrieved_context=['TP. Hồ Chí Minh là thành phố lớn', 'Hà Nội là thủ đô của Việt Nam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc965f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def jaccard(a, b): \n",
    "    A = set(a.lower().split()) \n",
    "    B = set(b.lower().split()) \n",
    "    return len(A & B) / len(A | B) if len(A | B) > 0 else 0\n",
    "\n",
    "def evaluate_jaccard(referenced_context: List[str], retrieved_context: List[str], threshold=0.3):\n",
    "    \"\"\"\n",
    "    Calculate Precision, Recall, F1-Score, and MRR using Jaccard similarity.\n",
    "    \"\"\"\n",
    "    referenced_set = list(set(referenced_context))\n",
    "    retrieved_set  = list(set(retrieved_context))\n",
    "\n",
    "    # --- Precision / Recall / F1 ---\n",
    "    used = set()   # retrieved indices already matched\n",
    "    tp = 0\n",
    "\n",
    "    for ref in referenced_set:\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "\n",
    "        for i, ret in enumerate(retrieved_set):\n",
    "            if i in used:\n",
    "                continue\n",
    "\n",
    "            score = jaccard(ref, ret)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = i\n",
    "\n",
    "        if best_score >= threshold:\n",
    "            tp += 1\n",
    "            used.add(best_match)\n",
    "\n",
    "    precision = tp / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
    "    recall    = tp / len(referenced_set) if len(referenced_set) > 0 else 0\n",
    "    f1_score  = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # --- MRR ---\n",
    "    reciprocal_rank = 0.0\n",
    "    for rank, ret in enumerate(retrieved_set, start=1):\n",
    "        max_score = max(jaccard(ref, ret) for ref in referenced_set)\n",
    "        if max_score >= threshold:\n",
    "            reciprocal_rank = 1 / rank\n",
    "            break  # only first relevant item counts\n",
    "\n",
    "    return {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1_score,\n",
    "        'MRR': reciprocal_rank\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c374e45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Precision': 1.0, 'Recall': 0.6666666666666666, 'F1-Score': 0.8, 'MRR': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_jaccard(referenced_context=['Hà Nội là thủ đô của Việt Nam', 'Hà Nội có Lăng Bác', 'TP. Hồ Chí Minh ở miền Nam'], retrieved_context=['TP. Hồ Chí Minh là thành phố lớn', 'Hà Nội là thủ đô của Việt Nam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49744b",
   "metadata": {},
   "source": [
    "### Use RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d72b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    LLMContextRecall\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "metrics = [\n",
    "    LLMContextRecall(),\n",
    "    ContextPrecision()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad65eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"user_input\": \"What is CPU scheduling in operating systems?\",\n",
    "        \"retrieved_contexts\": [\n",
    "            \"CPU scheduling determines which process runs on the CPU.\",\n",
    "            \"Schedulers improve efficiency and throughput.\"\n",
    "        ],\n",
    "        \"reference\": \"CPU scheduling is the method by which the OS selects a process to run next.\",\n",
    "        'synthesizer_name': 'gpt-3.5-turbo'\n",
    "    },\n",
    "    {\n",
    "        \"user_input\": \"Explain what virtual memory is.\",\n",
    "        \"retrieved_contexts\": [\n",
    "            \"Virtual memory uses disk space to extend RAM.\",\n",
    "            \"It allows executing programs larger than physical memory.\"\n",
    "        ],\n",
    "        \"reference\": \"Virtual memory allows a computer to compensate for physical memory limitations using disk space.\",\n",
    "        'synthesizer_name': 'gpt-3.5-turbo'\n",
    "    }\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfd227",
   "metadata": {},
   "source": [
    "#### Use OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f11c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "\n",
    "df[\"retrieved_contexts\"] = df[\"retrieved_contexts\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else [x]\n",
    ")\n",
    "\n",
    "hf_ds = HFDataset.from_pandas(df)\n",
    "results = evaluate(hf_ds, metrics=metrics, llm=llm, embeddings=embeddings)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c114be",
   "metadata": {},
   "source": [
    "#### Or use HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "# LLM\n",
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Embeddings\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate(hf_ds, metrics=metrics, llm=llm, embeddings=embeddings)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phobert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
