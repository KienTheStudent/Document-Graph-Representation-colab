{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95780ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Documents\\PROJECT\\GroupProject\\Taxelith\\.venv_rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ADMIN\\Documents\\PROJECT\\GroupProject\\Taxelith\\.venv_rag\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import shutil\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# LangChain Retrievers\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Vector Stores\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Transformers for PhoBERT\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Document Processing\n",
    "from markitdown import MarkItDown\n",
    "from langchain_docling.loader import DoclingLoader, ExportType\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, EasyOcrOptions\n",
    "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
    "\n",
    "# Evaluation\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import context_recall, context_precision\n",
    "from datasets import Dataset\n",
    "\n",
    "# Suppress warnings\n",
    "logging.getLogger(\"docling\").setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701a102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Pipeline Config: Device=cpu, Chunk Size=500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CONFIGURATION CLASS\n",
    "class PipelineConfig(BaseModel):\n",
    "    chunk_size: int = 500\n",
    "    chunk_overlap: int = 50\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    bm25_weight: float = 0.5\n",
    "    dense_weight: float = 0.5\n",
    "    top_k: int = 5\n",
    "\n",
    "config = PipelineConfig()\n",
    "print(f\"üöÄ Pipeline Config: Device={config.device}, Chunk Size={config.chunk_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066db639",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CUSTOM ENSEMBLE RETRIEVER (REPLACEMENT FOR DEPRECATED ONE)\n",
    "\n",
    "class CustomEnsembleRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    Custom implementation of Ensemble Retriever since it was removed from LangChain.\n",
    "    Combines multiple retrievers using weighted reciprocal rank fusion.\n",
    "    \"\"\"\n",
    "    retrievers: List[BaseRetriever]\n",
    "    weights: List[float]\n",
    "    k: int = 5\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "    \n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: Optional[CallbackManagerForRetrieverRun] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Retrieve documents from all retrievers and merge using weighted RRF.\"\"\"\n",
    "        \n",
    "        # Get results from all retrievers\n",
    "        all_results = []\n",
    "        for retriever in self.retrievers:\n",
    "            try:\n",
    "                docs = retriever.invoke(query)\n",
    "                all_results.append(docs)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Retriever failed: {e}\")\n",
    "                all_results.append([])\n",
    "        \n",
    "        # Reciprocal Rank Fusion with weights\n",
    "        doc_scores = {}\n",
    "        for docs, weight in zip(all_results, self.weights):\n",
    "            for rank, doc in enumerate(docs):\n",
    "                doc_id = doc.page_content[:100]  # Use content snippet as ID\n",
    "                if doc_id not in doc_scores:\n",
    "                    doc_scores[doc_id] = {\"doc\": doc, \"score\": 0}\n",
    "                # RRF formula: weight / (rank + 60)\n",
    "                doc_scores[doc_id][\"score\"] += weight / (rank + 60)\n",
    "        \n",
    "        # Sort by score and return top k\n",
    "        sorted_docs = sorted(doc_scores.values(), key=lambda x: x[\"score\"], reverse=True)\n",
    "        return [item[\"doc\"] for item in sorted_docs[:self.k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1214fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# METADATA FLATTENING UTILITY\n",
    "\n",
    "def flatten_metadata(metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Flatten nested metadata to make it compatible with ChromaDB and Pinecone.\n",
    "    Only keep str, int, float, bool values.\n",
    "    \"\"\"\n",
    "    flat = {}\n",
    "    for key, value in metadata.items():\n",
    "        # Skip complex nested structures\n",
    "        if isinstance(value, (dict, list)):\n",
    "            # Convert to JSON string representation\n",
    "            import json\n",
    "            try:\n",
    "                flat[key] = json.dumps(value)[:500]  # Limit length\n",
    "            except:\n",
    "                flat[key] = str(value)[:500]\n",
    "        elif isinstance(value, (str, int, float, bool)):\n",
    "            flat[key] = value\n",
    "        elif value is None:\n",
    "            flat[key] = \"none\"\n",
    "        else:\n",
    "            flat[key] = str(value)[:500]\n",
    "    \n",
    "    return flat\n",
    "\n",
    "\n",
    "def clean_documents(docs: List[Document]) -> List[Document]:\n",
    "    \"\"\"Clean documents by flattening their metadata.\"\"\"\n",
    "    cleaned = []\n",
    "    for doc in docs:\n",
    "        cleaned_doc = Document(\n",
    "            page_content=doc.page_content,\n",
    "            metadata=flatten_metadata(doc.metadata)\n",
    "        )\n",
    "        cleaned.append(cleaned_doc)\n",
    "    return cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19f9be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading vinai/phobert-base on cpu...\n",
      "‚úÖ Embedding models initialized\n",
      "‚ö†Ô∏è  Note: Pinecone requires OpenAI embeddings - skipping if quota exceeded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# EMBEDDING MODELS\n",
    "\n",
    "class PhoBertLangChainWrapper(Embeddings):\n",
    "    \"\"\"PhoBERT Embedding Model for Vietnamese text (768 dimensions)\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"vinai/phobert-base\", device=None):\n",
    "        self.device = device or config.device\n",
    "        print(f\"üì¶ Loading {model_name} on {self.device}...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Apply mean pooling over token embeddings\"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "            input_mask_expanded.sum(1), min=1e-9\n",
    "        )\n",
    "\n",
    "    def _embed(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Embed a list of texts\"\"\"\n",
    "        encoded_input = self.tokenizer(\n",
    "            texts, \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=256, \n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        \n",
    "        # Mean pooling + normalization\n",
    "        sentence_embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        \n",
    "        return sentence_embeddings.cpu().numpy().tolist()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self._embed(texts)\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self._embed([text])[0]\n",
    "\n",
    "\n",
    "# Initialize embedding models\n",
    "phobert_embedding = PhoBertLangChainWrapper(device=config.device)\n",
    "\n",
    "print(\"‚úÖ Embedding models initialized\")\n",
    "print(\"‚ö†Ô∏è  Note: Pinecone requires OpenAI embeddings - skipping if quota exceeded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5609333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHUNKING STRATEGIES\n",
    "\n",
    "def get_chunks_hybrid(file_path: str) -> List[Document]:\n",
    "    \"\"\"Hybrid Chunking using Docling (Structure-aware)\"\"\"\n",
    "    print(f\"üìÑ Running Hybrid Chunking on {file_path}...\")\n",
    "    try:\n",
    "        pipeline_options = PdfPipelineOptions(\n",
    "            do_ocr=True, \n",
    "            ocr_options=EasyOcrOptions(lang=['vi'])\n",
    "        )\n",
    "        loader = DoclingLoader(\n",
    "            file_path=file_path,\n",
    "            export_type=ExportType.DOC_CHUNKS,\n",
    "            chunker=HybridChunker(\n",
    "                chunk_size=config.chunk_size, \n",
    "                chunk_overlap=config.chunk_overlap\n",
    "            )\n",
    "        )\n",
    "        chunks = list(loader.load())\n",
    "        \n",
    "        # CRITICAL: Clean metadata for ChromaDB compatibility\n",
    "        chunks = clean_documents(chunks)\n",
    "        \n",
    "        print(f\"   ‚úÖ Generated {len(chunks)} chunks (metadata cleaned)\")\n",
    "        return chunks\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Hybrid chunking failed: {e}\")\n",
    "        print(f\"   üîÑ Falling back to Recursive chunking...\")\n",
    "        return get_chunks_recursive(file_path)\n",
    "\n",
    "\n",
    "def get_chunks_recursive(file_path: str) -> List[Document]:\n",
    "    \"\"\"Recursive Character Text Splitter (General purpose)\"\"\"\n",
    "    print(f\"üìÑ Running Recursive Chunking on {file_path}...\")\n",
    "    mk = MarkItDown()\n",
    "    markdown_text = mk.convert(file_path).markdown\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=config.chunk_size, \n",
    "        chunk_overlap=config.chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "    doc_obj = Document(page_content=markdown_text, metadata={'source': file_path})\n",
    "    chunks = splitter.split_documents([doc_obj])\n",
    "    print(f\"   ‚úÖ Generated {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_chunks_custom(file_path: str, words_per_chunk: int = 200) -> List[Document]:\n",
    "    \"\"\"Custom Word-based Chunking (Simple word windows)\"\"\"\n",
    "    print(f\"üìÑ Running Custom Chunking on {file_path}...\")\n",
    "    mk = MarkItDown()\n",
    "    text = mk.convert(file_path).markdown\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), words_per_chunk):\n",
    "        chunk_text = \" \".join(words[i:i+words_per_chunk])\n",
    "        chunks.append(Document(\n",
    "            page_content=chunk_text, \n",
    "            metadata={'source': file_path, 'chunk_id': str(i)}  # Ensure string type\n",
    "        ))\n",
    "    \n",
    "    print(f\"   ‚úÖ Generated {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Strategy mapping\n",
    "chunking_strategies = {\n",
    "    \"Hybrid\": get_chunks_hybrid,\n",
    "    \"Recursive\": get_chunks_recursive,\n",
    "    \"Custom\": get_chunks_custom\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d80396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VECTOR DATABASE CREATION\n",
    "\n",
    "def create_chroma_retriever(docs: List[Document], embeddings: Embeddings, collection_name: str):\n",
    "    \"\"\"Create Chroma vector store retriever (Local, persistent) - WITH FILE LOCK FIX\"\"\"\n",
    "    print(f\"   üîß Creating Chroma retriever for {collection_name}...\")\n",
    "    persist_dir = f\"./chroma_db_{collection_name}\"\n",
    "    \n",
    "    # Force remove old collection to avoid file locks\n",
    "    try:\n",
    "        if os.path.exists(persist_dir):\n",
    "            shutil.rmtree(persist_dir)\n",
    "            time.sleep(0.5)  # Wait for file system\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Warning during cleanup: {e}\")\n",
    "    \n",
    "    try:\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            collection_name=collection_name,\n",
    "            persist_directory=persist_dir\n",
    "        )\n",
    "        return vectorstore.as_retriever(search_kwargs={\"k\": config.top_k})\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Chroma creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_faiss_retriever(docs: List[Document], embeddings: Embeddings, collection_name: str):\n",
    "    \"\"\"Create FAISS vector store retriever (Local, in-memory)\"\"\"\n",
    "    print(f\"   üîß Creating FAISS retriever for {collection_name}...\")\n",
    "    try:\n",
    "        vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "        return vectorstore.as_retriever(search_kwargs={\"k\": config.top_k})\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå FAISS creation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_pinecone_retriever(docs: List[Document], embeddings: Embeddings, collection_name: str):\n",
    "    \"\"\"Create Pinecone vector store retriever (Cloud-based) - SKIP IF NO QUOTA\"\"\"\n",
    "    print(f\"   üîß Creating Pinecone retriever for {collection_name}...\")\n",
    "    print(f\"   ‚ö†Ô∏è  Skipping Pinecone (OpenAI quota exceeded - use PhoBERT for local testing)\")\n",
    "    return None  # Comment this out if you have valid OpenAI API key\n",
    "    \n",
    "    # Uncomment below if you have OpenAI credits:\n",
    "    \"\"\"\n",
    "    index_name = \"raglegal\"\n",
    "    namespace = f\"exp_{collection_name}_{int(time.time())}\"\n",
    "    \n",
    "    try:\n",
    "        openai_embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        vectorstore = PineconeVectorStore.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=openai_embedding,\n",
    "            index_name=index_name,\n",
    "            namespace=namespace\n",
    "        )\n",
    "        return vectorstore.as_retriever(search_kwargs={\"k\": config.top_k})\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Pinecone Error: {e}\")\n",
    "        return None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# Database strategy mapping (all use PhoBERT now)\n",
    "db_strategies = {\n",
    "    \"Chroma\": (create_chroma_retriever, phobert_embedding),\n",
    "    \"FAISS\": (create_faiss_retriever, phobert_embedding),\n",
    "    # \"Pinecone\": (create_pinecone_retriever, phobert_embedding)  # Commented out\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef7a100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üöÄ STARTING PIPELINE: 3 Chunking √ó 2 VectorDB = 6 Experiments\n",
      "================================================================================\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìã CHUNKING STRATEGY: Hybrid\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìÑ Running Hybrid Chunking on C:/Users/ADMIN/Documents/PROJECT/GroupProject/Taxelith/document/luat_thue_ttdb_2025.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 16:40:29,988 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "2025-11-26 16:40:30,096 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,397 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,430 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\ADMIN\\Documents\\PROJECT\\GroupProject\\Taxelith\\.venv_rag\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,431 [RapidOCR] main.py:53: Using C:\\Users\\ADMIN\\Documents\\PROJECT\\GroupProject\\Taxelith\\.venv_rag\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,611 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,616 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\ADMIN\\Documents\\PROJECT\\GroupProject\\Taxelith\\.venv_rag\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,617 [RapidOCR] main.py:53: Using C:\\Users\\ADMIN\\Documents\\PROJECT\\GroupProject\\Taxelith\\.venv_rag\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,681 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,703 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\ADMIN\\Documents\\PROJECT\\GroupProject\\Taxelith\\.venv_rag\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2025-11-26 16:40:31,703 [RapidOCR] main.py:53: Using C:\\Users\\ADMIN\\Documents\\PROJECT\\GroupProject\\Taxelith\\.venv_rag\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1126 > 512). Running this sequence through the model will result in indexing errors\n",
      "2025-11-26 16:41:15,968 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Generated 59 chunks (metadata cleaned)\n",
      "   ‚úÖ BM25 retriever created\n",
      "\n",
      "   üî¨ Experiment: Hybrid_Chroma\n",
      "   üîß Creating Chroma retriever for Hybrid_Chroma...\n",
      "   ‚úÖ Hybrid retriever created (BM25: 0.5, Dense: 0.5)\n",
      "\n",
      "   üî¨ Experiment: Hybrid_FAISS\n",
      "   üîß Creating FAISS retriever for Hybrid_FAISS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 16:41:49,125 - INFO - Loading faiss with AVX2 support.\n",
      "2025-11-26 16:41:49,568 - INFO - Successfully loaded faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Hybrid retriever created (BM25: 0.5, Dense: 0.5)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìã CHUNKING STRATEGY: Recursive\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìÑ Running Recursive Chunking on C:/Users/ADMIN/Documents/PROJECT/GroupProject/Taxelith/document/luat_thue_ttdb_2025.pdf...\n",
      "   ‚úÖ Generated 55 chunks\n",
      "   ‚úÖ BM25 retriever created\n",
      "\n",
      "   üî¨ Experiment: Recursive_Chroma\n",
      "   üîß Creating Chroma retriever for Recursive_Chroma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 16:41:50,730 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Hybrid retriever created (BM25: 0.5, Dense: 0.5)\n",
      "\n",
      "   üî¨ Experiment: Recursive_FAISS\n",
      "   üîß Creating FAISS retriever for Recursive_FAISS...\n",
      "   ‚úÖ Hybrid retriever created (BM25: 0.5, Dense: 0.5)\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìã CHUNKING STRATEGY: Custom\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "üìÑ Running Custom Chunking on C:/Users/ADMIN/Documents/PROJECT/GroupProject/Taxelith/document/luat_thue_ttdb_2025.pdf...\n",
      "   ‚úÖ Generated 23 chunks\n",
      "   ‚úÖ BM25 retriever created\n",
      "\n",
      "   üî¨ Experiment: Custom_Chroma\n",
      "   üîß Creating Chroma retriever for Custom_Chroma...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 16:42:22,041 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Hybrid retriever created (BM25: 0.5, Dense: 0.5)\n",
      "\n",
      "   üî¨ Experiment: Custom_FAISS\n",
      "   üîß Creating FAISS retriever for Custom_FAISS...\n",
      "   ‚úÖ Hybrid retriever created (BM25: 0.5, Dense: 0.5)\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Pipeline Complete: 6 retrievers created\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# PIPELINE EXECUTION (6 Combinations: 3 Chunking √ó 2 VectorDB)\n",
    "\n",
    "FILE_PATH = 'C:/Users/ADMIN/Documents/PROJECT/GroupProject/Taxelith/document/luat_thue_ttdb_2025.pdf'\n",
    "\n",
    "results_store = {}  # Store all retrievers for evaluation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üöÄ STARTING PIPELINE: 3 Chunking √ó 2 VectorDB = 6 Experiments\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for chunk_name, chunk_func in chunking_strategies.items():\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"üìã CHUNKING STRATEGY: {chunk_name}\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    # Step 1: Generate chunks\n",
    "    docs = chunk_func(FILE_PATH)\n",
    "    \n",
    "    if not docs:\n",
    "        print(f\"   ‚ùå No chunks generated, skipping {chunk_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Step 2: Create BM25 retriever (sparse, keyword-based)\n",
    "    try:\n",
    "        bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "        bm25_retriever.k = config.top_k\n",
    "        print(f\"   ‚úÖ BM25 retriever created\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå BM25 creation failed: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Step 3: Loop through vector databases\n",
    "    for db_name, (db_func, embedding_model) in db_strategies.items():\n",
    "        experiment_id = f\"{chunk_name}_{db_name}\"\n",
    "        print(f\"\\n   üî¨ Experiment: {experiment_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Create dense retriever (semantic, embedding-based)\n",
    "            dense_retriever = db_func(docs, embedding_model, collection_name=experiment_id)\n",
    "            \n",
    "            if dense_retriever is None:\n",
    "                print(f\"   ‚ùå Skipped {experiment_id} (Database creation failed)\")\n",
    "                continue\n",
    "            \n",
    "            # Create hybrid retriever using CUSTOM implementation\n",
    "            ensemble_retriever = CustomEnsembleRetriever(\n",
    "                retrievers=[bm25_retriever, dense_retriever],\n",
    "                weights=[config.bm25_weight, config.dense_weight],\n",
    "                k=config.top_k\n",
    "            )\n",
    "            \n",
    "            results_store[experiment_id] = {\n",
    "                \"retriever\": ensemble_retriever,\n",
    "                \"docs\": docs,\n",
    "                \"chunk_strategy\": chunk_name,\n",
    "                \"db_strategy\": db_name\n",
    "            }\n",
    "            print(f\"   ‚úÖ Hybrid retriever created (BM25: {config.bm25_weight}, Dense: {config.dense_weight})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Failed {experiment_id}: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Pipeline Complete: {len(results_store)} retrievers created\")\n",
    "print(f\"{'='*80}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d35bd860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION FUNCTIONS\n",
    "def calculate_simple_recall(retriever, test_questions: List[str], ground_truths: List[str]) -> float:\n",
    "    \"\"\"Simple recall: Check if ground truth text appears in retrieved docs\"\"\"\n",
    "    hits = 0\n",
    "    for question, truth in zip(test_questions, ground_truths):\n",
    "        try:\n",
    "            retrieved_docs = retriever.invoke(question)\n",
    "            found = any(truth.lower() in doc.page_content.lower() for doc in retrieved_docs)\n",
    "            if found:\n",
    "                hits += 1\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Retrieval failed for question: {e}\")\n",
    "    return hits / len(test_questions) if test_questions else 0.0\n",
    "\n",
    "\n",
    "def calculate_precision_at_k(retriever, test_questions: List[str], ground_truths: List[str], k: int = 5) -> float:\n",
    "    \"\"\"Calculate precision: ratio of relevant docs in top-k\"\"\"\n",
    "    precisions = []\n",
    "    for question, truth in zip(test_questions, ground_truths):\n",
    "        try:\n",
    "            retrieved_docs = retriever.invoke(question)[:k]\n",
    "            relevant_count = sum(1 for doc in retrieved_docs if truth.lower() in doc.page_content.lower())\n",
    "            precisions.append(relevant_count / k if k > 0 else 0)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Retrieval failed: {e}\")\n",
    "            precisions.append(0)\n",
    "    return sum(precisions) / len(precisions) if precisions else 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test data - REPLACE WITH YOUR REAL TEST SET\n",
    "test_questions = [\n",
    "    \"Thu·∫ø ti√™u th·ª• ƒë·∫∑c bi·ªát l√† g√¨?\",\n",
    "    \"Thu·∫ø su·∫•t v·ªõi bia l√† bao nhi√™u?\",\n",
    "    \"ƒê·ªëi t∆∞·ª£ng n√†o ch·ªãu thu·∫ø ti√™u th·ª• ƒë·∫∑c bi·ªát?\"\n",
    "]\n",
    "\n",
    "test_ground_truths = [\n",
    "    \"thu·∫ø gi√°n thu\",  # Key phrase to search for\n",
    "    \"65%\",\n",
    "    \"h√†ng h√≥a\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä STARTING EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for exp_id, data in results_store.items():\n",
    "    print(f\"\\nüß™ Evaluating: {exp_id}\")\n",
    "    retriever = data['retriever']\n",
    "    \n",
    "    try:\n",
    "        # Calculate metrics\n",
    "        recall = calculate_simple_recall(retriever, test_questions, test_ground_truths)\n",
    "        precision = calculate_precision_at_k(retriever, test_questions, test_ground_truths, k=config.top_k)\n",
    "        \n",
    "        print(f\"   ‚úÖ Recall@{config.top_k}: {recall:.4f}\")\n",
    "        print(f\"   ‚úÖ Precision@{config.top_k}: {precision:.4f}\")\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            \"Experiment\": exp_id,\n",
    "            \"Chunking\": data['chunk_strategy'],\n",
    "            \"VectorDB\": data['db_strategy'],\n",
    "            \"Recall\": recall,\n",
    "            \"Precision\": precision,\n",
    "            \"F1_Score\": 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Evaluation failed: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f18bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RESULTS SUMMARY & EXPORT\n",
    "\n",
    "if evaluation_results:\n",
    "    df_results = pd.DataFrame(evaluation_results)\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    df_results = df_results.sort_values('F1_Score', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìà EVALUATION RESULTS (Sorted by F1 Score)\")\n",
    "    print(\"=\"*80)\n",
    "    print(df_results.to_string(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = f\"retrieval_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df_results.to_csv(output_file, index=False)\n",
    "    print(f\"\\nüíæ Results saved to: {output_file}\")\n",
    "    \n",
    "    # Find best configuration\n",
    "    if len(df_results) > 0:\n",
    "        best_config = df_results.iloc[0]\n",
    "        print(\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "        print(f\"   Experiment: {best_config['Experiment']}\")\n",
    "        print(f\"   Chunking: {best_config['Chunking']}\")\n",
    "        print(f\"   VectorDB: {best_config['VectorDB']}\")\n",
    "        print(f\"   Recall: {best_config['Recall']:.4f}\")\n",
    "        print(f\"   Precision: {best_config['Precision']:.4f}\")\n",
    "        print(f\"   F1 Score: {best_config['F1_Score']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No evaluation results generated\")\n",
    "    print(\"   Please check if any retrievers were successfully created\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phobert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
